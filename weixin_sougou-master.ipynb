{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\ProgramData\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fbc24ed8b424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mcookies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_cookies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_account_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[1;31m#print(weixin_search(\"简书\",cookies))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-fbc24ed8b424>\u001b[0m in \u001b[0;36mget_account_info\u001b[0;34m(open_id, link, cookies)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0minfo_box\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'#weixinname'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0maccount_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0maccount_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'account'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h4 span'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'：'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import selenium\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "import random\n",
    "\n",
    "BASE_URL = 'http://weixin.sogou.com'\n",
    "\n",
    "UA = \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"\n",
    "\n",
    "def get_html(url):\n",
    "    dcap = dict(DesiredCapabilities.PHANTOMJS)\n",
    "    dcap[\"phantomjs.page.settings.userAgent\"] = (\n",
    "        UA\n",
    "    )\n",
    "    dcap[\"takesScreenshot\"] = (False)\n",
    "    #t0 = time.time()\n",
    "    try:\n",
    "        driver = webdriver.PhantomJS(desired_capabilities=dcap, service_args=['--load-images=no'])\n",
    "        driver.set_page_load_timeout(240)\n",
    "        driver.command_executor._commands['executePhantomScript'] = ('POST', '/session/$sessionId/phantom/execute')\n",
    "\n",
    "        driver.execute('executePhantomScript', {'script': '''\n",
    "            var page = this; // won't work otherwise\n",
    "            page.onResourceRequested = function(requestData, request) {\n",
    "                if ((/http:\\/\\/.+?\\.css/gi).test(requestData['url']) || requestData['Content-Type'] == 'text/css') {\n",
    "                    console.log('The url of the request is matching. Aborting: ' + requestData['url']);\n",
    "                    request.abort();\n",
    "                }\n",
    "            }\n",
    "            ''', 'args': []})\n",
    "    except selenium.common.exceptions.WebDriverException:\n",
    "        return None\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        html = driver.page_source\n",
    "    except Exception as e:\n",
    "        html = None\n",
    "        logging.error(e)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return html\n",
    "\n",
    "def get_html_direct(url,cookies=None):\n",
    "    if not cookies:\n",
    "        cookies = update_cookies()\n",
    "    headers = {\"User-Agent\": UA}\n",
    "    r = requests.get(url, headers=headers, cookies=cookies, timeout=20)\n",
    "    return r.text\n",
    "\n",
    "def get_account_info(open_id=None, link=None, cookies=None):\n",
    "    url = None\n",
    "    if open_id:\n",
    "        url = BASE_URL + '/gzh?openid=' + open_id\n",
    "    if link:\n",
    "        url = link\n",
    "    #html = get_html(url)\n",
    "    html = get_html_direct(url, cookies=cookies)\n",
    "    #print(html)\n",
    "    if not html:\n",
    "        return None\n",
    "    soup = BeautifulSoup(html)\n",
    "    info_box = soup.select('#weixinname')[0].parent\n",
    "    account_info = {}\n",
    "    account_info['account'] = info_box.select('h4 span')[0].text.split('：')[1].strip()\n",
    "    account_info['name'] = info_box.select('#weixinname')[0].text\n",
    "    account_info['address'] = url\n",
    "    account_info['description'] = info_box.select('.sp-txt')[0].text\n",
    "    img_list = soup.select('.pos-box img')\n",
    "    account_info['logo'] = soup.select(\".img-box img\")[0]['src']\n",
    "    account_info['qr_code'] = img_list[1]['src']\n",
    "    return account_info\n",
    "\n",
    "\n",
    "def parse_list(open_id=None, link=None):\n",
    "    if open_id:\n",
    "        url = BASE_URL + '/gzh?openid=' + open_id\n",
    "    elif link:\n",
    "        url = link\n",
    "    else:\n",
    "        return None\n",
    "    html = get_html(url)\n",
    "    if not html:\n",
    "        return None\n",
    "    soup = BeautifulSoup(html)\n",
    "    ls = soup.select('#wxbox .txt-box')\n",
    "    link_list = []\n",
    "    for item in ls:\n",
    "        item_dict = {}\n",
    "        item_dict['title'] = item.a.text\n",
    "        item_dict['link'] = item.a['href']\n",
    "        link_list.append(item_dict)\n",
    "    return link_list\n",
    "\n",
    "\n",
    "def parse_essay(link):\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\"User-Agent\": UA})\n",
    "    try:\n",
    "        r = s.get(link)\n",
    "        html = r.text\n",
    "        soup = BeautifulSoup(html)\n",
    "        essay = {}\n",
    "        p = re.compile(r'\\?wx_fmt.+?\\\"')\n",
    "        content = str(soup.select(\"#js_content\")[0]).replace('data-src', 'src')\n",
    "        essay['content'] = re.sub(p, '\"', content)\n",
    "        essay['name'] = soup.select('#post-user')[0].text\n",
    "        essay['date'] = soup.select('#post-date')[0].text\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    return essay\n",
    "\n",
    "\n",
    "def weixin_search(name, cookies=None):\n",
    "    url = BASE_URL + '/weixin?query=' + name\n",
    "    #html = get_html(url)\n",
    "    html = get_html_direct(url, cookies=cookies)\n",
    "    print(html)\n",
    "    soup = BeautifulSoup(html)\n",
    "    ls = soup.select(\"._item\")\n",
    "    search_list = []\n",
    "    for item in ls:\n",
    "        account_info = {}\n",
    "        account_info['account'] = item.select('h4 span')[0].text.split('：')[1].strip()\n",
    "        account_info['name'] = item.select('.txt-box h3')[0].text\n",
    "        account_info['address'] = BASE_URL + item['href']\n",
    "        account_info['open_id'] = item['href'].split('openid=')[1]\n",
    "        account_info['description'] = item.select('.sp-txt')[0].text\n",
    "        account_info['logo'] = item.select('.img-box img')[0]['src']\n",
    "        try:\n",
    "            account_info['latest_title'] = item.select('.sp-txt a')[0].text\n",
    "            account_info['latest_link'] = item.select('.sp-txt a')[0]['href']\n",
    "        except IndexError:\n",
    "            pass\n",
    "        search_list.append(account_info)\n",
    "        #print(account_info)\n",
    "    return search_list\n",
    "\n",
    "def update_cookies():\n",
    "    s = requests.Session()\n",
    "    headers = {\"User-Agent\": UA}\n",
    "    s.headers.update(headers)\n",
    "    url = BASE_URL + '/weixin?query=123'\n",
    "    r = s.get(url)\n",
    "    if 'SNUID' not in s.cookies:\n",
    "        p = re.compile(r'(?<=SNUID=)\\w+')\n",
    "        s.cookies['SNUID'] = p.findall(r.text)[0]\n",
    "        suv = ''.join([str(int(time.time()*1000000) + random.randint(0, 1000))])\n",
    "        s.cookies['SUV'] = suv\n",
    "    return s.cookies\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    open_id = 'oIWsFt3nvJ2jaaxm9UOB_LUos02k'\n",
    "    #print(weixin_search('简书'))\n",
    "    cookies = update_cookies()\n",
    "    t0 = time.time()\n",
    "    print(get_account_info(open_id,cookies=cookies))\n",
    "    #print(weixin_search(\"简书\",cookies))\n",
    "    t1 = time.time()\n",
    "    print(parse_list(open_id))\n",
    "    t2 = time.time()\n",
    "    print(parse_essay('http://mp.weixin.qq.com/s?__biz=MjM5NjM4OTAyMA==&mid=205212599&idx=4&sn=6a1de7a7532ba0bcbc633c253b61916f&3rd=MzA3MDU4NTYzMw==&scene=6#rd'))\n",
    "    t3 = time.time()\n",
    "    print(t1-t0, t2-t1, t3-t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
